{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project442.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOaIaUvTQd2v5Csk4weVFFN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndvaughn/dUnet/blob/main/project442.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk5iOriozVx"
      },
      "source": [
        "!pip install torchsummary\n",
        "import shutil\n",
        "import argparse\n",
        "import zipfile\n",
        "import hashlib\n",
        "import pickle\n",
        "import requests\n",
        "from IPython.display import clear_output \n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "from matplotlib import image\n",
        "import glob as glob\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62VRUerdQX_P"
      },
      "source": [
        "DEBUG = False\n",
        "DETECT_SBST = True \n",
        "TOP_K = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUyjRRp0pPaz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eDYzNRW-3XQ"
      },
      "source": [
        "# some helper functions to download the dataset\n",
        "# this code comes mainly from gluoncv.utils\n",
        "def check_sha1(filename, sha1_hash):\n",
        "    \"\"\"Check whether the sha1 hash of the file content matches the expected hash.\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename : str\n",
        "        Path to the file.\n",
        "    sha1_hash : str\n",
        "        Expected sha1 hash in hexadecimal digits.\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "        Whether the file content matches the expected hash.\n",
        "    \"\"\"\n",
        "    sha1 = hashlib.sha1()\n",
        "    with open(filename, 'rb') as f:\n",
        "        while True:\n",
        "            data = f.read(1048576)\n",
        "            if not data:\n",
        "                break\n",
        "            sha1.update(data)\n",
        "\n",
        "    sha1_file = sha1.hexdigest()\n",
        "    l = min(len(sha1_file), len(sha1_hash))\n",
        "    return sha1.hexdigest()[0:l] == sha1_hash[0:l]\n",
        "\n",
        "def download(url, path=None, overwrite=False, sha1_hash=None):\n",
        "    \"\"\"Download an given URL\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        URL to download\n",
        "    path : str, optional\n",
        "        Destination path to store downloaded file. By default stores to the\n",
        "        current directory with same name as in url.\n",
        "    overwrite : bool, optional\n",
        "        Whether to overwrite destination file if already exists.\n",
        "    sha1_hash : str, optional\n",
        "        Expected sha1 hash in hexadecimal digits. Will ignore existing file when hash is specified\n",
        "        but doesn't match.\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The file path of the downloaded file.\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        fname = url.split('/')[-1]\n",
        "    else:\n",
        "        path = os.path.expanduser(path)\n",
        "        if os.path.isdir(path):\n",
        "            fname = os.path.join(path, url.split('/')[-1])\n",
        "        else:\n",
        "            fname = path\n",
        "\n",
        "    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n",
        "        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n",
        "        if not os.path.exists(dirname):\n",
        "            os.makedirs(dirname)\n",
        "\n",
        "        print('Downloading %s from %s...'%(fname, url))\n",
        "        r = requests.get(url, stream=True)\n",
        "        if r.status_code != 200:\n",
        "            raise RuntimeError(\"Failed downloading url %s\"%url)\n",
        "        total_length = r.headers.get('content-length')\n",
        "        with open(fname, 'wb') as f:\n",
        "            if total_length is None: # no content length header\n",
        "                for chunk in r.iter_content(chunk_size=1024):\n",
        "                    if chunk: # filter out keep-alive new chunks\n",
        "                        f.write(chunk)\n",
        "            else:\n",
        "                total_length = int(total_length)\n",
        "                for chunk in tqdm(r.iter_content(chunk_size=1024),\n",
        "                                  total=int(total_length / 1024. + 0.5),\n",
        "                                  unit='KB', unit_scale=False, dynamic_ncols=True):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        if sha1_hash and not check_sha1(fname, sha1_hash):\n",
        "            raise UserWarning('File {} is downloaded but the content hash does not match. ' \\\n",
        "                              'The repo may be outdated or download may be incomplete. ' \\\n",
        "                              'If the \"repo_url\" is overridden, consider switching to ' \\\n",
        "                              'the default repo.'.format(fname))\n",
        "\n",
        "    return fname\n",
        "\n",
        "def download_ade(path, overwrite=False):\n",
        "\n",
        "    \"\"\"Download ADE20K\n",
        "    Parameters\n",
        "    ----------\n",
        "    path : str\n",
        "      Location of the downloaded files.\n",
        "    overwrite : bool, optional\n",
        "      Whether to overwrite destination file if already exists.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "    _AUG_DOWNLOAD_URLS = [\n",
        "      ('http://data.csail.mit.edu/places/ADEchallenge/ADEChallengeData2016.zip', '219e1696abb36c8ba3a3afe7fb2f4b4606a897c7'),\n",
        "      ('http://data.csail.mit.edu/places/ADEchallenge/release_test.zip', 'e05747892219d10e9243933371a497e905a4860c'),]\n",
        "    download_dir = os.path.join(path, 'downloads')\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.mkdir(download_dir)\n",
        "    for url, checksum in _AUG_DOWNLOAD_URLS:\n",
        "        filename = download(url, path=download_dir, overwrite=overwrite, sha1_hash=checksum)\n",
        "        # extract\n",
        "        with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
        "            zip_ref.extractall(path=path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93ZILSz7_YGD"
      },
      "source": [
        "root = \"/content/\"\n",
        "dataset_path = root + \"ADEChallengeData2016/images/\"\n",
        "training_data = \"training/\"\n",
        "val_data = \"validation/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63WUVsyQ_YyP"
      },
      "source": [
        "download_ade(root, overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwNpyMSXB6Hv"
      },
      "source": [
        "TRAINSET_SIZE = len(glob.glob(dataset_path + training_data + \"*.jpg\"))\n",
        "print(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n",
        "\n",
        "VALSET_SIZE = len(glob.glob(dataset_path + val_data + \"*.jpg\"))\n",
        "print(f\"The Validation Dataset contains {VALSET_SIZE} images.\")\n",
        "\n",
        "N_CLASSES = 151"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGEF-t-6uh6L"
      },
      "source": [
        "def normal_init(m, mean, std):\n",
        "    \"\"\"\n",
        "    Helper function. Initialize model parameter with given mean and std.\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hd08xWdAhvx"
      },
      "source": [
        "class Ade20k(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir: the directory of the dataset\n",
        "            split: \"train\" or \"val\"\n",
        "            transform: pytorch transformations.\n",
        "        \"\"\"\n",
        "\n",
        "        self.transform = transform\n",
        "        self.resize = transforms.Resize((256,256), interpolation=Image.NEAREST)\n",
        "        self.ToTensor = transforms.ToTensor()\n",
        "        self.normalize = transforms.Compose([\n",
        "                                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        if split == 'train':\n",
        "          self.img_files = glob.glob(root_dir + '/training/*.jpg')\n",
        "        else:\n",
        "          self.img_files = glob.glob(root_dir + '/validation/*.jpg')\n",
        "        self.mask_files = [w.replace('images', 'annotations').replace('jpg','png') for w in self.img_files]\n",
        "\n",
        "        if DEBUG:\n",
        "          self.img_files = self.img_files[:500]\n",
        "          self.mask_files = self.mask_files[:500]\n",
        "          # print(np.shape(np.asarray(Image.open(self.mask_files[0]))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_files[idx])\n",
        "        img = self.resize(img)\n",
        "        img = np.asarray(img)\n",
        "        if img.ndim != 3:\n",
        "          img = np.expand_dims(img,2)\n",
        "          img = np.repeat(img,3,2)\n",
        "        img = self.ToTensor(img)\n",
        "        img = self.normalize(img)\n",
        "\n",
        "        label = Image.open(self.mask_files[idx])\n",
        "        label = self.resize(label)\n",
        "        label = np.asarray(label)\n",
        "        if DETECT_SBST:\n",
        "          #need to copy since download is read-only\n",
        "          temp = np.copy(label)\n",
        "          temp[temp > TOP_K] = 0\n",
        "          temp = torch.from_numpy(temp).to(torch.long)\n",
        "          return img, temp\n",
        "        label = torch.from_numpy(label).to(torch.long)\n",
        "        #Uncomment below line to move to BCE loss \n",
        "        # label = F.one_hot(label, num_classes=151).permute((2,0,1)).float()\n",
        "        \n",
        "        return img, label\n",
        "\n",
        "\n",
        "tr_dt = Ade20k(dataset_path, split='train')\n",
        "te_dt = Ade20k(dataset_path, split='val')\n",
        "train_loader = DataLoader(tr_dt, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(te_dt, batch_size=5, shuffle=False)\n",
        "\n",
        "# Make sure that you have 1,000 training images and 100 testing images before moving on\n",
        "print('Number of training images {}, number of testing images {}'.format(len(tr_dt), len(te_dt)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrw61-WFC_eT"
      },
      "source": [
        "#Sample Output used for visualization\n",
        "test = test_loader.__iter__().__next__()\n",
        "img_size = 256\n",
        "fixed_y_ = test[1].cuda()\n",
        "fixed_x_ = test[0].cuda()\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))\n",
        "print(fixed_y_.shape)\n",
        "\n",
        "# plot sample image\n",
        "fig, axes = plt.subplots(2, 2)\n",
        "axes = np.reshape(axes, (4, ))\n",
        "for i in range(4):\n",
        "    example = train_loader.__iter__().__next__()[0][i].numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std = np.array([0.5, 0.5, 0.5])\n",
        "    example = std * example + mean\n",
        "    axes[i].imshow(example)\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfHJebwDlIbJ"
      },
      "source": [
        "class Unet(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self):\n",
        "          super(Unet, self).__init__()\n",
        "\n",
        "          self.conv1 = nn.Conv2d(3, 64, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.ReLU1 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv2 = nn.Conv2d(64, 128, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN2 = nn.BatchNorm2d(128)\n",
        "          self.ReLU2 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv3 = nn.Conv2d(128, 256, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN3 = nn.BatchNorm2d(256)\n",
        "          self.ReLU3 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv4 = nn.Conv2d(256, 512, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN4 = nn.BatchNorm2d(512)\n",
        "          self.ReLU4 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv5 = nn.Conv2d(512, 512, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN5 = nn.BatchNorm2d(512)\n",
        "          self.ReLU5 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv6 = nn.Conv2d(512, 512, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN6 = nn.BatchNorm2d(512)\n",
        "          self.ReLU6 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv7 = nn.Conv2d(512, 512, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN7 = nn.BatchNorm2d(512)\n",
        "          self.ReLU7 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          self.conv8 = nn.Conv2d(512, 512, kernel_size=(4,4), stride=2, padding=1)\n",
        "          self.BN8 = nn.BatchNorm2d(512)\n",
        "          self.ReLU8 = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "\n",
        "\n",
        "          self.conv9 = nn.ConvTranspose2d(512, 512, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN9 = nn.BatchNorm2d(512)\n",
        "\n",
        "          self.conv10 = nn.ConvTranspose2d(1024, 512, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN10 = nn.BatchNorm2d(512)\n",
        "\n",
        "          self.conv11 = nn.ConvTranspose2d(1024, 512, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN11 = nn.BatchNorm2d(512)\n",
        "\n",
        "          self.conv12 = nn.ConvTranspose2d(1024, 512, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN12 = nn.BatchNorm2d(512)\n",
        "\n",
        "          self.conv13 = nn.ConvTranspose2d(1024, 256, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN13 = nn.BatchNorm2d(256)\n",
        "\n",
        "          self.conv14 = nn.ConvTranspose2d(512, 128, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN14 = nn.BatchNorm2d(128)\n",
        "\n",
        "          self.conv15 = nn.ConvTranspose2d(256, 64, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          self.BN15 = nn.BatchNorm2d(64)\n",
        "\n",
        "          self.conv16 = nn.ConvTranspose2d(128, 3, kernel_size=(4,4), stride=(2,2), padding=(1,1))\n",
        "          if not DETECT_SBST:\n",
        "            self.conv17 = nn.ConvTranspose2d(3, 151, kernel_size=(1,1), stride=(1,1))\n",
        "          else:\n",
        "            self.conv17 = nn.ConvTranspose2d(3, TOP_K + 1, kernel_size=(1,1), stride=(1,1))\n",
        "          self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        o1 = self.conv1(input)\n",
        "        o = nn.functional.leaky_relu(o1, negative_slope=0.2)\n",
        "\n",
        "\n",
        "        o2 = self.conv2(o)\n",
        "        o = self.BN2(o2)\n",
        "        o = nn.functional.leaky_relu(o, negative_slope=0.2)\n",
        "\n",
        "\n",
        "        o3 = self.conv3(o)\n",
        "        o = self.BN3(o3)\n",
        "        o = nn.functional.leaky_relu(o, negative_slope=0.2)\n",
        "\n",
        "\n",
        "        o4 = self.conv4(o)\n",
        "        o = self.BN4(o4)\n",
        "        o = nn.functional.leaky_relu(o, negative_slope=0.2)\n",
        "\n",
        "\n",
        "        o5 = self.conv5(o)\n",
        "        o = self.BN5(o5)\n",
        "        o = nn.functional.leaky_relu(o, negative_slope=0.2)\n",
        "\n",
        "\n",
        "        o6 = self.conv6(o)\n",
        "        o = self.BN6(o6)\n",
        "        o = nn.functional.leaky_relu(o, negative_slope=0.2)\n",
        "\n",
        "        \n",
        "        o7 = self.conv7(o)\n",
        "        o = self.BN7(o7)\n",
        "        o = nn.functional.leaky_relu(o, negative_slope=0.2)\n",
        "\n",
        "\n",
        "        o8 = self.conv8(o)\n",
        "        o = nn.functional.leaky_relu(o8, negative_slope=0.2)\n",
        "\n",
        "        o = torch.tanh(o)\n",
        "\n",
        "\n",
        "        o9 = self.conv9(o)\n",
        "        o = self.BN9(o9)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "        o10 = self.conv10(torch.cat((o,o7), dim=1))\n",
        "        o = self.BN10(o10)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "\n",
        "        o11 = self.conv11(torch.cat((o6,o), dim=1))\n",
        "        o = self.BN11(o11)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "  \n",
        "        o12 = self.conv12(torch.cat((o5,o), dim=1))\n",
        "        o = self.BN12(o12)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "        o13 = self.conv13(torch.cat((o4,o), dim=1))\n",
        "        o = self.BN13(o13)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "\n",
        "        o14 = self.conv14(torch.cat((o3,o), dim=1))\n",
        "        o = self.BN14(o14)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "    \n",
        "        o15 = self.conv15(torch.cat((o2,o), dim=1))\n",
        "        o = self.BN15(o15)\n",
        "        o = torch.nn.functional.relu(o, inplace=True)\n",
        "\n",
        "        o = self.conv16(torch.cat((o1,o), dim=1))\n",
        "        o = self.conv17(o)\n",
        "\n",
        "        return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShX_L61pldAJ"
      },
      "source": [
        "class UnetPP(nn.Module):\n",
        "  def __init__(self):\n",
        "          super(UnetPP, self).__init__()\n",
        "\n",
        "          self.ReLU = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          # self.enc00 = nn.Conv2d(4, 64//2, kernel_size=(4,4))\n",
        "          # self.enc01 = nn.Conv2d(64//2, 64//4, kernel_size=(4,4), dilation=4)\n",
        "          # self.enc02 = nn.Conv2d(64//4, 64//8, kernel_size=(4,4), dilation=6)\n",
        "          # self.enc03 = nn.Conv2d(64//8, 64//16, kernel_size=(4,4), dilation=9)\n",
        "          # self.enc04 = nn.Conv2d(64//16, 64//16, kernel_size=(4,4), dilation=12)\n",
        "          self.enc00 = nn.Conv2d(3,64,kernel_size=(3,3), stride=1, padding=1) #This keeps dimensionailty\n",
        "\n",
        "\n",
        "          #ENCODE BLOCK 1\n",
        "          self.max1 = nn.MaxPool2d((2,2), stride=2) # this should decrease dimensionality by factor of 2\n",
        "          self.enc10 = nn.Conv2d(64, 128//2, kernel_size=(3,3), stride=1, padding=1) \n",
        "          self.enc11 = nn.Conv2d(128//2, 128//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc12 = nn.Conv2d(128//4, 128//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc13 = nn.Conv2d(128//8, 128//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc14 = nn.Conv2d(128//16, 128//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          #ENCODE BLOCK 2\n",
        "          self.max2 = nn.MaxPool2d(2,stride=2) # decrease by factor of 2\n",
        "          self.enc20 = nn.Conv2d(128, 256//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.enc21 = nn.Conv2d(256//2, 256//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc22 = nn.Conv2d(256//4, 256//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc23 = nn.Conv2d(256//8, 256//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc24 = nn.Conv2d(256//16, 256//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          #ENCODE BLOCK 3\n",
        "          self.max3 = nn.MaxPool2d(2, stride=2)\n",
        "          self.enc30 = nn.Conv2d(256, 512//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.enc31 = nn.Conv2d(512//2, 512//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc32 = nn.Conv2d(512//4, 512//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc33 = nn.Conv2d(512//8, 512//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc34 = nn.Conv2d(512//16, 512//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          #ENCODE BLOCK 4\n",
        "          self.max4 = nn.MaxPool2d(2, stride=2)\n",
        "          self.enc40 = nn.Conv2d(512, 512//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.enc41 = nn.Conv2d(512//2, 512//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc42 = nn.Conv2d(512//4, 512//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc43 = nn.Conv2d(512//8, 512//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc44 = nn.Conv2d(512//16, 512//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          #SKIP LAYER (2,1)\n",
        "          self.skip21 = nn.Conv2d(512+512, 512+512, kernel_size=(3,3), stride=1, padding=1) #input is concatination of up(enc3) and (enc2)\n",
        "\n",
        "          #SKIP LAYER (1,1) and (1,2)\n",
        "          self.skip11 = nn.Conv2d(512+256, 512+256, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.skip12 = nn.Conv2d(512+512+256+(256+512), 512+512+256+(256+512), kernel_size=(3,3), stride=1, padding=1)\n",
        "\n",
        "          #SKIP LAYER (0,1), (0,2), (0,3)\n",
        "          self.skip01 = nn.Conv2d(256+128, 256+128, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.skip02 = nn.Conv2d(128+(256+128)+(512+256),128+(256+128)+(512+256), kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.skip03 = nn.Conv2d(128+(256+128)+128+(256+128)+(256+512)+512+512+256+(256+512),3840, kernel_size=(3,3), stride=1, padding=1)\n",
        "\n",
        "\n",
        "          #DECODE BLOCK 1\n",
        "          self.up0 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec00 = nn.ConvTranspose2d(1024+1024, 256//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.dec01 = nn.ConvTranspose2d(256//2, 256//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec02 = nn.ConvTranspose2d(256//4, 256//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec03 = nn.ConvTranspose2d(256//8, 256//16, kernel_size=(3,3), dilation=9, padding=9)\n",
        "          self.dec04 = nn.ConvTranspose2d(256//16, 256//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          #DECODE BLOCK 2\n",
        "          self.up1 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec10 = nn.ConvTranspose2d(3328, 128//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.dec11 = nn.ConvTranspose2d(128//2, 128//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec12 = nn.ConvTranspose2d(128//4, 128//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec13 = nn.ConvTranspose2d(128//8, 128//16, kernel_size=(3,3), dilation=9, padding=9)\n",
        "          self.dec14 = nn.ConvTranspose2d(128//16, 128//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          #DECODE BLOCK 3\n",
        "          self.up2 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec20 = nn.ConvTranspose2d(5760, 64//2, kernel_size=(3,3), padding=1, stride=1)\n",
        "          self.dec21 = nn.ConvTranspose2d(64//2, 64//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec22 = nn.ConvTranspose2d(64//4, 64//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec23 = nn.ConvTranspose2d(64//8, 64//16, kernel_size=(3,3), dilation=9, padding=9)\n",
        "          self.dec24 = nn.ConvTranspose2d(64//16, 64//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          #DECODE BLOCK 4\n",
        "          self.up3 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec30 = nn.ConvTranspose2d(128, 64//2, kernel_size=(3,3), padding=1, stride=1)\n",
        "          self.dec31 = nn.ConvTranspose2d(64//2, 64//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec32 = nn.ConvTranspose2d(64//4, 64//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec33 = nn.ConvTranspose2d(64//8, 64//16, kernel_size=(3,3), dilation=9,padding=9)\n",
        "          self.dec34 = nn.ConvTranspose2d(64//16, 64//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "\n",
        "          #CLASSIFICATION LAYER\n",
        "          self.up4 = nn.Upsample(scale_factor=(2,2))\n",
        "          if not DETECT_SBST:\n",
        "            self.dec4 = nn.ConvTranspose2d(64,151,kernel_size=(1,1))\n",
        "          else:\n",
        "            self.dec4 = nn.Conv2d(64,TOP_K + 1,kernel_size=(1,1))\n",
        "          \n",
        "          self.sigmoid = nn.Sigmoid()\n",
        "          \n",
        "    \n",
        "  # weight_init\n",
        "  def weight_init(self, mean, std):\n",
        "      for m in self._modules:\n",
        "          normal_init(self._modules[m], mean, std)\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "      #ENCODE\n",
        "\n",
        "      x=input\n",
        "      x = self.enc00(input)\n",
        "      out0_x = self.ReLU(x)\n",
        "\n",
        "      x = self.max1(out0_x)\n",
        "      x_1 = self.ReLU(self.enc10(x))\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc11(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc12(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc13(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc14(x))\n",
        "      out1_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.max2(out1_x)\n",
        "      x_1 = self.ReLU(self.enc20(x))\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc21(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc22(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc23(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc24(x))\n",
        "      out2_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.max3(out2_x)\n",
        "      x_1 = self.ReLU(self.enc30(x))\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc31(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc32(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc33(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc34(x))\n",
        "      out3_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "   \n",
        "      x = self.max3(out3_x)\n",
        "      x_1 = self.ReLU(self.enc40(x))\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc41(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc42(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc43(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc44(x))\n",
        "      out4_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      #SKIP CONNECTIONS\n",
        "\n",
        "      print(out3_x.shape, out4_x.shape)\n",
        "      x21 = self.ReLU(self.skip21(torch.cat((self.up0(out4_x), out3_x),dim=1)))\n",
        "\n",
        "      x11 = self.ReLU(self.skip11(torch.cat((self.up0(out3_x),out2_x),dim=1)))\n",
        "      x12 = self.ReLU(self.skip12(torch.cat((self.up0(x21), torch.cat((out2_x,x11),dim=1)),dim=1)))\n",
        "\n",
        "      x01 = self.ReLU(self.skip01(torch.cat((self.up0(out2_x),out1_x),dim=1)))\n",
        "      x02 = self.ReLU(self.skip02(torch.cat((self.up0(x11), torch.cat((out1_x,x01),dim=1)),dim=1)))\n",
        "      x03 = self.ReLU(self.skip03(torch.cat((self.up0(x12), torch.cat((out1_x,x01,x02),dim=1)),dim=1)))\n",
        "\n",
        "\n",
        "      #DECODE\n",
        "\n",
        "      x = self.up0(out4_x)\n",
        "      x_1 = self.ReLU(self.dec00(torch.cat((out3_x,x21,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec01(x_1))\n",
        "      x_3 = self.ReLU(self.dec02(x_2))\n",
        "      x_4 = self.ReLU(self.dec03(x_3))\n",
        "      x_5 = self.ReLU(self.dec04(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up1(x)\n",
        "      x_1 = self.ReLU(self.dec10(torch.cat((out2_x,x11,x12,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec11(x_1))\n",
        "      x_3 = self.ReLU(self.dec12(x_2))\n",
        "      x_4 = self.ReLU(self.dec13(x_3))\n",
        "      x_5 = self.ReLU(self.dec14(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up2(x)\n",
        "      x_1 = self.ReLU(self.dec20(torch.cat((out1_x,x01,x02,x03,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec21(x_1))\n",
        "      x_3 = self.ReLU(self.dec22(x_2))\n",
        "      x_4 = self.ReLU(self.dec23(x_3))\n",
        "      x_5 = self.ReLU(self.dec24(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up3(x)\n",
        "      x_1 = self.ReLU(self.dec30(torch.cat((out0_x,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec31(x_1))\n",
        "      x_3 = self.ReLU(self.dec32(x_2))\n",
        "      x_4 = self.ReLU(self.dec33(x_3))\n",
        "      x_5 = self.ReLU(self.dec34(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.dec4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBDAsWgvEt7S"
      },
      "source": [
        "class DialatedUnet(nn.Module):\n",
        "  def __init__(self):\n",
        "          super(DialatedUnet, self).__init__()\n",
        "\n",
        "          self.ReLU = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "          # self.enc00 = nn.Conv2d(4, 64//2, kernel_size=(4,4))\n",
        "          # self.enc01 = nn.Conv2d(64//2, 64//4, kernel_size=(4,4), dilation=4)\n",
        "          # self.enc02 = nn.Conv2d(64//4, 64//8, kernel_size=(4,4), dilation=6)\n",
        "          # self.enc03 = nn.Conv2d(64//8, 64//16, kernel_size=(4,4), dilation=9)\n",
        "          # self.enc04 = nn.Conv2d(64//16, 64//16, kernel_size=(4,4), dilation=12)\n",
        "          self.enc00 = nn.Conv2d(3,64,kernel_size=(3,3), stride=1, padding=1) #This keeps dimensionailty\n",
        "\n",
        "          self.max1 = nn.MaxPool2d((2,2), stride=2) # this should decrease dimensionality by factor of 2\n",
        "          self.enc10 = nn.Conv2d(64, 128//2, kernel_size=(3,3), stride=1, padding=1) \n",
        "          self.enc11 = nn.Conv2d(128//2, 128//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc12 = nn.Conv2d(128//4, 128//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc13 = nn.Conv2d(128//8, 128//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc14 = nn.Conv2d(128//16, 128//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          self.max2 = nn.MaxPool2d(2,stride=2) # decrease by factor of 2\n",
        "          self.enc20 = nn.Conv2d(128, 256//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.enc21 = nn.Conv2d(256//2, 256//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc22 = nn.Conv2d(256//4, 256//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc23 = nn.Conv2d(256//8, 256//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc24 = nn.Conv2d(256//16, 256//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          self.max3 = nn.MaxPool2d(2, stride=2)\n",
        "          self.enc30 = nn.Conv2d(256, 512//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.enc31 = nn.Conv2d(512//2, 512//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc32 = nn.Conv2d(512//4, 512//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc33 = nn.Conv2d(512//8, 512//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc34 = nn.Conv2d(512//16, 512//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          self.max4 = nn.MaxPool2d(2, stride=2)\n",
        "          self.enc40 = nn.Conv2d(512, 512//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.enc41 = nn.Conv2d(512//2, 512//4, kernel_size=(3,3), dilation=3)\n",
        "          self.enc42 = nn.Conv2d(512//4, 512//8, kernel_size=(3,3), dilation=6)\n",
        "          self.enc43 = nn.Conv2d(512//8, 512//16, kernel_size=(3,3), dilation=9)\n",
        "          self.enc44 = nn.Conv2d(512//16, 512//16, kernel_size=(3,3), dilation=12)\n",
        "\n",
        "          self.up0 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec00 = nn.ConvTranspose2d(1024, 256//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.dec01 = nn.ConvTranspose2d(256//2, 256//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec02 = nn.ConvTranspose2d(256//4, 256//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec03 = nn.ConvTranspose2d(256//8, 256//16, kernel_size=(3,3), dilation=9, padding=9)\n",
        "          self.dec04 = nn.ConvTranspose2d(256//16, 256//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          self.up1 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec10 = nn.ConvTranspose2d(512, 128//2, kernel_size=(3,3), stride=1, padding=1)\n",
        "          self.dec11 = nn.ConvTranspose2d(128//2, 128//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec12 = nn.ConvTranspose2d(128//4, 128//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec13 = nn.ConvTranspose2d(128//8, 128//16, kernel_size=(3,3), dilation=9, padding=9)\n",
        "          self.dec14 = nn.ConvTranspose2d(128//16, 128//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          self.up2 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec20 = nn.ConvTranspose2d(256, 64//2, kernel_size=(3,3), padding=1, stride=1)\n",
        "          self.dec21 = nn.ConvTranspose2d(64//2, 64//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec22 = nn.ConvTranspose2d(64//4, 64//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec23 = nn.ConvTranspose2d(64//8, 64//16, kernel_size=(3,3), dilation=9, padding=9)\n",
        "          self.dec24 = nn.ConvTranspose2d(64//16, 64//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          self.up3 = nn.Upsample(scale_factor=(2,2))\n",
        "          self.dec30 = nn.ConvTranspose2d(128, 64//2, kernel_size=(3,3), padding=1, stride=1)\n",
        "          self.dec31 = nn.ConvTranspose2d(64//2, 64//4, kernel_size=(3,3), dilation=3, padding=3)\n",
        "          self.dec32 = nn.ConvTranspose2d(64//4, 64//8, kernel_size=(3,3), dilation=6, padding=6)\n",
        "          self.dec33 = nn.ConvTranspose2d(64//8, 64//16, kernel_size=(3,3), dilation=9,padding=9)\n",
        "          self.dec34 = nn.ConvTranspose2d(64//16, 64//16, kernel_size=(3,3), dilation=12, padding=12)\n",
        "\n",
        "          self.up4 = nn.Upsample(scale_factor=(2,2))\n",
        "\n",
        "          if not DETECT_SBST:\n",
        "            self.dec4 = nn.ConvTranspose2d(64,151,kernel_size=(1,1))\n",
        "          else:\n",
        "            self.dec4 = nn.Conv2d(64,TOP_K + 1,kernel_size=(1,1))\n",
        "          \n",
        "          self.sigmoid = nn.Sigmoid()\n",
        "          \n",
        "    \n",
        "  # weight_init\n",
        "  def weight_init(self, mean, std):\n",
        "      for m in self._modules:\n",
        "          normal_init(self._modules[m], mean, std)\n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "      x=input\n",
        "      x = self.enc00(input)\n",
        "      out0_x = self.ReLU(x)\n",
        "\n",
        "      x = self.max1(out0_x)\n",
        "      x_1 = self.ReLU(self.enc10(x))\n",
        "      #print(x_1.shape)\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc11(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc12(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc13(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc14(x))\n",
        "      out1_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.max2(out1_x)\n",
        "      x_1 = self.ReLU(self.enc20(x))\n",
        "      # print(x_1.shape)\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc21(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc22(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc23(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc24(x))\n",
        "      out2_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.max3(out2_x)\n",
        "      x_1 = self.ReLU(self.enc30(x))\n",
        "      # print(x_1.shape)\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc31(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc32(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc33(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc34(x))\n",
        "      out3_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "   \n",
        "      x = self.max3(out3_x)\n",
        "      x_1 = self.ReLU(self.enc40(x))\n",
        "      # print(x_1.shape)\n",
        "      x = F.pad(x_1,(3,3,3,3))\n",
        "      x_2 = self.ReLU(self.enc41(x))\n",
        "      x = F.pad(x_2,(6,6,6,6))\n",
        "      x_3 = self.ReLU(self.enc42(x))\n",
        "      x = F.pad(x_3,(9,9,9,9))\n",
        "      x_4 = self.ReLU(self.enc43(x))\n",
        "      x = F.pad(x_4,(12,12,12,12))\n",
        "      x_5 = self.ReLU(self.enc44(x))\n",
        "      out4_x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up0(out4_x)\n",
        "      x_1 = self.ReLU(self.dec00(torch.cat((out3_x,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec01(x_1))\n",
        "      x_3 = self.ReLU(self.dec02(x_2))\n",
        "      x_4 = self.ReLU(self.dec03(x_3))\n",
        "      x_5 = self.ReLU(self.dec04(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up1(x)\n",
        "      x_1 = self.ReLU(self.dec10(torch.cat((out2_x,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec11(x_1))\n",
        "      x_3 = self.ReLU(self.dec12(x_2))\n",
        "      x_4 = self.ReLU(self.dec13(x_3))\n",
        "      x_5 = self.ReLU(self.dec14(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up2(x)\n",
        "      x_1 = self.ReLU(self.dec20(torch.cat((out1_x,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec21(x_1))\n",
        "      x_3 = self.ReLU(self.dec22(x_2))\n",
        "      x_4 = self.ReLU(self.dec23(x_3))\n",
        "      x_5 = self.ReLU(self.dec24(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.up3(x)\n",
        "      x_1 = self.ReLU(self.dec30(torch.cat((out0_x,x),dim=1)))\n",
        "      x_2 = self.ReLU(self.dec31(x_1))\n",
        "      x_3 = self.ReLU(self.dec32(x_2))\n",
        "      x_4 = self.ReLU(self.dec33(x_3))\n",
        "      x_5 = self.ReLU(self.dec34(x_4))\n",
        "      x = torch.cat((x_1, x_2, x_3, x_4, x_5),dim=1)\n",
        "\n",
        "      x = self.dec4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xge-XUnmUiIN"
      },
      "source": [
        "dice_loss was experimented with but ultimately unused, there is currently no code which uses dice_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR7XxkLssPY4"
      },
      "source": [
        "def dice_loss(pred, target):\n",
        "    \"\"\"This definition generalize to real valued pred and target vector.\n",
        "This should be differentiable.\n",
        "    pred: tensor with first dimension as batch\n",
        "    target: tensor with first dimension as batch\n",
        "    \"\"\"\n",
        "\n",
        "    smooth = 1.\n",
        "\n",
        "    # have to use contiguous since they may from a torch.view op\n",
        "    iflat = pred.contiguous().view(-1)\n",
        "    tflat = target.contiguous().view(-1)\n",
        "    intersection = (iflat * tflat).sum()\n",
        "\n",
        "    A_sum = torch.sum(tflat * iflat)\n",
        "    B_sum = torch.sum(tflat * tflat)\n",
        "    \n",
        "    return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biOL9KVyoFQ1"
      },
      "source": [
        "\n",
        "# BCE_loss = nn.BCELoss().cuda()\n",
        "# L1_loss = nn.L1Loss().cuda()\n",
        "CE_loss = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "\n",
        "\n",
        "def train(G, num_epochs = 20):\n",
        "    hist_G_losses = []\n",
        "\n",
        "    G_optimizer = optim.Adam(G.parameters(), lr=0.0003, betas=(0.5,0.999))\n",
        "\n",
        "    print('training start!')\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Start training epoch %d' % (epoch + 1))\n",
        "        G_losses = []\n",
        "        val_losses = []\n",
        "        epoch_start_time = time.time()\n",
        "        num_iter = 0\n",
        "        for x_, y_ in tqdm(train_loader):\n",
        "            \n",
        "            x_, y_ = x_.cuda(), y_.cuda()\n",
        "      \n",
        "            # Train the generator\n",
        "            G.zero_grad()\n",
        "            G_result = G(x_)\n",
        "\n",
        "            G_train_loss = CE_loss(G_result, y_)\n",
        "            # G_train_loss = dice_loss(G_result, y_)\n",
        "            \n",
        "            G_train_loss.backward()\n",
        "            G_optimizer.step()\n",
        "            loss_G = G_train_loss.detach().item()\n",
        "\n",
        "            G_losses.append(loss_G)\n",
        "            hist_G_losses.append(loss_G)\n",
        "            num_iter += 1\n",
        "        \n",
        "        # for x_, y_ in tqdm(test_loader):\n",
        "        #   with torch.no_grad():\n",
        "        #     x_, y_ = x_.cuda(), y_.cuda()\n",
        "        #     G_result = G(x_)\n",
        "        #     G_val_loss = CE_loss(G_result, y_)\n",
        "        #     val_losses.append(G_val_loss.detach().item())\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print('[%d/%d] - using time: %.2f seconds' % ((epoch + 1), num_epochs, per_epoch_ptime))\n",
        "        print('loss of generator G: %.3f' % (torch.mean(torch.FloatTensor(G_losses))))\n",
        "        # print('Validation loss: %.3f' % (torch.mean(torch.FloatTensor(val_losses))))\n",
        "        # if epoch == 0 or (epoch + 1) % 2 == 0:\n",
        "        with torch.no_grad():\n",
        "                # print('TO DO: display image')\n",
        "                show_result(G, fixed_x_, fixed_y_, (epoch+1))\n",
        "                print(\"IOU after epochs \", mIOU(fixed_y_,G(fixed_x_),TOP_K+1))\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_ptime = end_time - start_time\n",
        "\n",
        "    return hist_G_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F1sQ-SStY4c"
      },
      "source": [
        "# Uncomment below networks to work with each model one at a time\n",
        "\n",
        "# Define network\n",
        "# unet = Unet()\n",
        "# unet.weight_init(mean=0.0, std=0.02)\n",
        "# unet.cuda()\n",
        "# unet.train()\n",
        "\n",
        "unetD = DialatedUnet()\n",
        "unetD.weight_init(mean=(0.0), std=0.02)\n",
        "unetD.cuda()\n",
        "unetD.train()\n",
        "\n",
        "# unetPP = UnetPP()\n",
        "# unetPP.weight_init(mean=(0.0), std=0.02)\n",
        "# unetPP.cuda()\n",
        "# unetPP.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj_RIOXCNtT3"
      },
      "source": [
        "def process_image(img):\n",
        "    if img.shape[0] == 3:\n",
        "      return (img.cpu().data.numpy().transpose(1, 2, 0) + 1) / 2\n",
        "    return (img.cpu().data.numpy() + 1) / 2\n",
        "\n",
        "def show_result(G, x_, y_, num_epoch):\n",
        "    predict_images = torch.argmax(G(x_),dim=1)\n",
        "    \n",
        "    fig, ax = plt.subplots(x_.size()[0], 3, figsize=(6,10))\n",
        "\n",
        "    for i in range(x_.size()[0]):\n",
        "        ax[i, 0].get_xaxis().set_visible(False)\n",
        "        ax[i, 0].get_yaxis().set_visible(False)\n",
        "        ax[i, 1].get_xaxis().set_visible(False)\n",
        "        ax[i, 1].get_yaxis().set_visible(False)\n",
        "        ax[i, 2].get_xaxis().set_visible(False)\n",
        "        ax[i, 2].get_yaxis().set_visible(False)\n",
        "        ax[i, 0].cla()\n",
        "        ax[i, 0].imshow(process_image(x_[i]))\n",
        "        ax[i, 1].cla()\n",
        "        ax[i, 1].imshow(process_image(predict_images[i]))\n",
        "        ax[i, 2].cla()\n",
        "        ax[i, 2].imshow(process_image(y_[i]))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    label_epoch = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0, label_epoch, ha='center')\n",
        "    label_input = 'Input'\n",
        "    fig.text(0.18, 1, label_input, ha='center')\n",
        "    label_output = 'Output'\n",
        "    fig.text(0.5, 1, label_output, ha='center')\n",
        "    label_truth = 'Ground truth'\n",
        "    fig.text(0.81, 1, label_truth, ha='center')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "SMOOTH = 1e-6\n",
        "\n",
        "def mIOU(label, pred, num_classes=TOP_K+1):\n",
        "    pred = F.softmax(pred, dim=1)              \n",
        "    pred = torch.argmax(pred, dim=1).squeeze(1)\n",
        "    iou_list = list()\n",
        "    present_iou_list = list()\n",
        "\n",
        "    pred = pred.view(-1)\n",
        "    label = label.view(-1)\n",
        "    # Note: Following for loop goes from 0 to (num_classes-1)\n",
        "    # and ignore_index is num_classes, thus ignore_index is\n",
        "    # not considered in computation of IoU.\n",
        "    for sem_class in range(num_classes):\n",
        "        pred_inds = (pred == sem_class)\n",
        "        target_inds = (label == sem_class)\n",
        "        if target_inds.long().sum().item() == 0:\n",
        "            iou_now = float('nan')\n",
        "        else: \n",
        "            intersection_now = (pred_inds[target_inds]).long().sum().item()\n",
        "            union_now = pred_inds.long().sum().item() + target_inds.long().sum().item() - intersection_now\n",
        "            iou_now = float(intersection_now) / float(union_now)\n",
        "            present_iou_list.append(iou_now)\n",
        "        iou_list.append(iou_now)\n",
        "    return np.mean(present_iou_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTlkfMZA1Gfl"
      },
      "source": [
        "**Train over dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ4TDmYivUee"
      },
      "source": [
        "#Uncomment below lines to train specific models\n",
        "\n",
        "# hist_G_L1_losses = train(unetPP, num_epochs = 3)\n",
        "hist_G_L1_losses = train(unetD, num_epochs = 15)\n",
        "# hist_G_L1_losses = train(unet, num_epochs = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVDdc_MnUaVt"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijGS7mwNUeNt"
      },
      "source": [
        "#Uncomment below lines to test specific models\n",
        "#results = results = unetPP(test)\n",
        "results = unetD(test)\n",
        "# results = unet(test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}